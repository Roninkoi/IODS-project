# Regression and model validation (week 2)

```{r message=FALSE, warning=FALSE, include=FALSE, results='hide'}
library(tidyverse)
library(knitr)
library(caret)
library(dplyr)
library(ggplot2)
library(GGally)
require(gridExtra)
require(docxtools)
```

Academic performance of students depends on many factors, which may be difficult
to measure and model accurately. The grade that a student gets may be attributable
to qualities of each student, e.g. motivation and previous knowledge, but the
exact relationship between these is not directly measurable.
Linear regression is one of the simplest ways
to quantify the relationship $f$ between predictor variables $X$ and a response variable $Y$
using a linear model $\hat Y = \hat f(X)$ [@islr]. By fitting a model to a data
set, we can determine the prediction $\hat Y$ by finding an estimate for $\hat f$.
If we have the appropriate data set available, it should be possible to roughly predict
how well a student performs using a model.

A data set on student learning was created for regression analysis. This data set
is based on student learning data^[http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt] collected in 2014-2015, which describes student learning attitudes and outcomes.
The data includes scores from surveys answered by the students, which are used
to construct variables that describe different aspects of learning.

An R script was created in [`data/create_learning2014.R`](https://github.com/Roninkoi/IODS-project/blob/master/data/create_learning2014.R) to generate the data set.
The raw data <!--is read from the URL using `read.table()`, and it--> contains 183 rows and 60 variables (described in more detail in the code). We include
student gender, age and exam points from the raw data in our data set. Additionally,
we create variables based on the description in the metadata^[http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS2-meta.txt]. The data
contains a number of questions $Q$ on different aspects of learning, which are
classified into groups: deep learning $Q_\text{D}$, surface learning $Q_\text{SU}$ 
and strategic learning $Q_\text{ST}$. Scores for each group are calculated by simply
taking the mean over each set of questions:
$\text{D} = \overline Q_\text{D}, \
\text{SU} = \overline Q_\text{SU}, \
\text{ST} = \overline Q_\text{ST}.$
This is done similarly for questions on attitude towards statistics, which we also include in
our data set. Now our data set consists of the 7 variables `gender`, `age`, `attitude`, `deep`, `stra`, `surf` and `points`. Finally, we filter the data such that it only
contains students whose exam points are larger than 0, which leaves us with 166 students The finalized data set
is written to `data/learning2014.csv`. <!--using `write_csv()`. This file is read 
back using `read_csv()` to check (using `str()` and `head()`) that the data has been written correctly.-->

After loading the data set, we plot the scatter matrix (figure \@ref(fig:sm))) for the data set to get an overview of how each
variable is correlated to each other. This figure contains histograms for each variable, as well as correlation between variables (scatter plots in lower half and coefficients in upper half). Summary statistics for each variable are
shown in table \@ref(tab:sum). There are 166 students in the data set (66% women), aged between 17 and 55 (median age 22). Almost all of the students are likely undergraduates, with few older people as shown by the sharply peaked age histogram in figure \@ref(fig:sm). Histograms for the learning variables are roughly normal, except for `attitude` and `points`. We can see that these distributions are not quite unimodal, with `attitude` having a peak at the lower end (for men) and `points` having peaks at both the lower and higher end. This suggests that there are many students that score low or high, with fewer students in between than might be expected.

In figure \@ref(fig:sm), we can look at the correlations between variables to determine which are most significant for the exam scores. We can see that the most strongly correlated variable is `attitude`, which has a positive correlation coefficient of 0.437 (marked by three stars, indicating high confidence level). This means that for high `attitude` scores, we are likely to see high exam scores as well. We can even see that the shape of the `points` histogram slightly resebles the shape of the `attitude` one. The second and third most significant correlations are with `stra` (positive) and `surf` (negative). This suggests that a high strategic learning score is likely to lead to high exam scores, while surface learning is not. These two scores are also negatively correlated with each other, meaning that they represent somewhat opposite learning attitudes. Similarly, we see that a `deep` learning score is very negatively correlated with `surf`.`

```{r sm, message=FALSE, warning=FALSE, fig.cap="Scatter matrix of the learning2014 data set.\\label{fig:sm}", fig.align="center"}
# load data set
learning14 <- read_csv("./data/learning2014.csv")

# plot scatter matrix of variables
sm <- ggpairs(learning14, mapping = aes(colour=gender, alpha=0.7), 
              lower = list(combo = wrap("facethist", bins = 20)))
sm
```

```{r sum, message=FALSE, warning=FALSE}
# get summaries for different variables
age_summary <- summary(learning14$age)
attitude_summary <- summary(learning14$attitude)
deep_summary <- summary(learning14$deep)
surf_summary <- summary(learning14$surf)
stra_summary <- summary(learning14$stra)
points_summary <- summary(learning14$points)

# calculate number of students
n_students <- nrow(learning14)
n_men <- nrow(filter(learning14, gender == "M"))
n_women <- nrow(filter(learning14, gender == "F"))

kable(data.frame("Age"=c(age_summary[1], age_summary[3], 
                         age_summary[4], age_summary[6]),
                 "Attitude"=c(attitude_summary[1], attitude_summary[3], 
                              attitude_summary[4], attitude_summary[6]),
                 "Deep"=c(deep_summary[1], deep_summary[3], 
                          deep_summary[4], deep_summary[6]), 
                 "Surface"=c(surf_summary[1], surf_summary[3], 
                             surf_summary[4], surf_summary[6]), 
                 "Strategic"=c(stra_summary[1], stra_summary[3], 
                               stra_summary[4], stra_summary[6]), 
                 "Points"=c(points_summary[1], points_summary[3], 
                            points_summary[4], points_summary[6])), 
                 caption = sprintf("Summary of different variables for %i students (%i men, %i women).", 
                                   n_students, n_men, n_women))
```

We want to fit a linear model $\hat f(x_1, x_2, x_3) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3$ to the `learning2014` data set with 3 variables. To choose these variables, we first start by fitting all 7 of them. Fitting the linear model is done by using the `lm()` function. A summary of the fit is generated to get the coefficient values, p-values and $R^2$. The variable with the largest p-value is removed from the model, and the model is fitted again. We continue this process of elimination until only 3 variables are left: `age`, `attitude` and `stra`. The model with all variables "All" and the model with 3 variables "Linear" are shown in table \@ref(tab:fe) with their coefficients, p-values and $R^2$. We can see that the 3 variable model has lower p-values for `age` and `attitude`.

The p-values reported for coefficients are calculated using the $t$-test. We want to minimize the p-values for the coefficients of our fit in order to establish statistical significance. For our model's coefficients to be statistically significant, we would typically like the $p$-value to be less than $5%$. Similarly, we want to maximize $R^2$, which is a measure of how good our model is at predicting the response based on the predictor variables (goodness of fit). It is defined using the errors of the fit as
$R^2 = 1 - \text{RSS} / \text{TSS}$, where $\text{RSS}$ is the residual sum of squares and $\text{TSS}$ is the total sum of squares [@islr]. By decreasing $SS_\text{res}$, we improve the fit and move $R^2$ closer to 1.

The linear model predicts the exam points using the equation
$\hat f = 10.90 - 0.09 \ \text{age} + 3.48 \ \text{attitude} + 1.00 \ \text{stra}$ (table \@ref(tab:fe)). The variable `attitude` is the most significant, followed by `stra`, accounting for the majority of the points. This suggests that positive attitudes towards the subject and strategic skills (organization, time management, ...) play a significant factor in positive learning outcomes, which is an expected association. Age correlates slightly negatively with points, suggesting that younger people are more likely to earn higher scores. It could be that older people have less time to devote to studying, but other factors may also contribute to this.

Only the `attitude` parameter in the linear model has a p-value of less 5%, while `strategy` is fairly close at 6% and `age` at 10%. The $R^2$ score for the linear model 0.218, so the model predicts about 22% of the score based on the chosen variables. The rest can't be explained using the model, so they are due to external factors not accounted for by the model.

If we don't constrain ourselves to a linear model, we can improve the fit while still only using 3 variables. We can extend the model to account for non-linear terms for the variables: $\hat f(x_1, x_2, x_3) = \beta_0 + \beta_1 x_1^i + \beta_2 x_2^j + \beta_3 x_3^k$. We can try to find a better fit by varying the powers $i, j, k$ and picking the model with the lowest $R^2$ value and desired p-value. By searching $i, j, k \in [0, 10]$, we found the best fit with $\text{age}^7+\text{attitude}+\text{stra}^5$. This model has a 5% significance level for all variables, as well as an $R^2$ coefficient of 0.26, as shown in table \@ref(tab:fe). However, this more complicated model is difficult to justfify for such a limited data set, as the linear model is simpler and performs adequately. Since the model doesn't have a test data set, problems of overfitting can be a concern.

Comparisons for exam point histograms are shown in figure \@ref(fig:fit). These figures show the real exam point data with the predicted distribution from the linear model and best fit model. As can be seen, the point distribution is reproduced quite well, with the mean of the distribution roughly correct. Behavior at the tails shows the positions of the peaks (low and high), but their magnitude is overestimated.

```{r fit, message=FALSE, warning=FALSE, fig.cap="Scatter matrix of the learning2014 data set.\\label{fig:hist}", fig.align="center"}
# plot histogram of points for data and fit
fit_hist <- function (data, fit, title) {
  points <- data.frame(Points=c(data$points, predict(fit)),
                     Distribution=c("Data", "Fit"))

  ggplot(points, aes(Points, fill = Distribution)) + 
    geom_density(alpha = 0.5) + ylab("Density") + ggtitle(title)
}

# first fit full 7 variable linear model and remove variables based on p-value
fit_full <- lm(points ~ ., data = learning14)
fit_full_summary <- summary(fit_full)

# linear model with 3 most significant variables: age, attitude, stra
fit_lin <- lm(points ~ age + attitude + stra, data = learning14)
fit_lin_summary <- summary(fit_lin)
hist1 <- fit_hist(learning14, fit_lin, title = "age + attitude + stra")

# try to find better model by fitting x^p with varying p
max_r2 <- 0
max_ijk <- c(0, 0, 0)
fit_best <- fit_lin
pn <- 10
for (i in 1:pn) {
  for (j in 1:pn) {
    for (k in 1:pn) {
      # fit model with 3 parameters and varying powers
      fit_ijk <- lm(points ~ I(age^i) + I(attitude^j) + I(stra^k), 
                    data = learning14)
      fit_summary <- summary(fit_ijk)
      
      # is p-value low enough for all parameters?
      good_p <- !any(array(summary(fit_ijk)$coefficients[,4]) > 0.05)
      
      # if r2 better than best found so far, accept this fit
      if (fit_summary$r.squared > max_r2 && good_p) {
        max_r2 <- fit_summary$r.squared
        max_ijk <- c(i, j, k)
        fit_best <- fit_ijk
      }
    }
  }
}

fit_best_summary <- summary(fit_best)
message("best fit: (", max_ijk[1], ", ", max_ijk[2], ", ", max_ijk[3], 
        "), R2: ", max_r2)

hist2 <- fit_hist(learning14, fit_best, 
                  sprintf("age^%i+attitude^%i+stra^%i", 
                          max_ijk[1], max_ijk[2], max_ijk[3]))

grid.arrange(hist1, hist2, ncol=2, respect=T)
```

```{r fe}
fe <- data.frame("Fit"=c("Intercept", "Intercept p-value", 
                         "Age", "Age p-value", 
                         "Attitude", "Attitude p-value", 
                         "Strategy", "Strategy p-value", "R^2"),
                 "All"=c(fit_full_summary$coefficients[1,1], 
                         fit_full_summary$coefficients[1,4], 
                         fit_full_summary$coefficients[2,1], 
                         fit_full_summary$coefficients[2,4], 
                         fit_full_summary$coefficients[3,1], 
                         fit_full_summary$coefficients[3,4], 
                         fit_full_summary$coefficients[4,1], 
                         fit_full_summary$coefficients[4,4], 
                         fit_full_summary$r.squared),
                 "Linear"=c(fit_lin_summary$coefficients[1,1], 
                            fit_lin_summary$coefficients[1,4], 
                            fit_lin_summary$coefficients[2,1], 
                            fit_lin_summary$coefficients[2,4], 
                            fit_lin_summary$coefficients[3,1], 
                            fit_lin_summary$coefficients[3,4], 
                            fit_lin_summary$coefficients[4,1], 
                            fit_lin_summary$coefficients[4,4], 
                            fit_lin_summary$r.squared),
                 "Best"=c(fit_best_summary$coefficients[1,1], 
                          fit_best_summary$coefficients[1,4], 
                          fit_best_summary$coefficients[2,1], 
                          fit_best_summary$coefficients[2,4], 
                          fit_best_summary$coefficients[3,1], 
                          fit_best_summary$coefficients[3,4], 
                          fit_best_summary$coefficients[4,1], 
                          fit_best_summary$coefficients[4,4], 
                          fit_best_summary$r.squared))
fe <- format_engr(fe)
kable(fe, caption = "Summary of different fits.")
```

Diagnostic plots were produced for the linear model, shown in figure \@ref(fig:diag). The model assumes that variance is constant for the data set. Based on the residuals vs. fitted values plot, this is not exactly correct. There is a slight change in the spread of the residuals with respect to the fitted value, which suggests that the assumption of constant variance is not true [@mabs]. Since the QQ residuals plot is not a straight line, but curved at the ends with a step on the left, this suggests that the data is not exactly normally distributed, but skewed, heavy-tailed at one end and bimodal. The residuals vs. leverage plot shows the significance of individual data points for the fit coefficients. Most random errors should have low leverage, but there are a few outliers in the data set with high leverage and residuals. Data points that are outliers could potentially be errors in the data set.

```{r diag, message=FALSE, warning=FALSE, fig.cap="Diagnostic plots for linear model.\\label{fig:diag}", fig.align="center"}
par(mfrow=c(1, 3))
plot(fit_lin, which=c(1, 2, 5))
```